{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/udlbook/udlbook/blob/main/Notebooks/Chap08/8_3_Double_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6chybAVFJW2"
      },
      "source": [
        "# **Notebook 8.3: Double Descent**\n",
        "\n",
        "This notebook investigates double descent as described in section 8.4 of the book.\n",
        "\n",
        "It uses the MNIST-1D database which can be found at https://github.com/greydanus/mnist1d\n",
        "\n",
        "Work through the cells below, running each cell in turn. In various places you will see the words \"TODO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n",
        "\n",
        "Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fn9BP5N5TguP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/greydanus/mnist1d\n",
            "  Cloning https://github.com/greydanus/mnist1d to /private/var/folders/v7/qwgf7b4121df_qjcj5g2n3w00000gn/T/pip-req-build-owig9uue\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/greydanus/mnist1d /private/var/folders/v7/qwgf7b4121df_qjcj5g2n3w00000gn/T/pip-req-build-owig9uue\n",
            "  Resolved https://github.com/greydanus/mnist1d to commit 7878d96082abd200c546a07a4101fa90b30fdf7e\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: requests in /Users/chengjunyuan/miniconda3/envs/udl/lib/python3.13/site-packages (from mnist1d==0.0.2.post16) (2.32.5)\n",
            "Requirement already satisfied: numpy in /Users/chengjunyuan/miniconda3/envs/udl/lib/python3.13/site-packages (from mnist1d==0.0.2.post16) (2.3.3)\n",
            "Requirement already satisfied: matplotlib in /Users/chengjunyuan/miniconda3/envs/udl/lib/python3.13/site-packages (from mnist1d==0.0.2.post16) (3.10.6)\n",
            "Requirement already satisfied: scipy in /Users/chengjunyuan/miniconda3/envs/udl/lib/python3.13/site-packages (from mnist1d==0.0.2.post16) (1.16.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/chengjunyuan/miniconda3/envs/udl/lib/python3.13/site-packages (from matplotlib->mnist1d==0.0.2.post16) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/chengjunyuan/miniconda3/envs/udl/lib/python3.13/site-packages (from matplotlib->mnist1d==0.0.2.post16) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/chengjunyuan/miniconda3/envs/udl/lib/python3.13/site-packages (from matplotlib->mnist1d==0.0.2.post16) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/chengjunyuan/miniconda3/envs/udl/lib/python3.13/site-packages (from matplotlib->mnist1d==0.0.2.post16) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/chengjunyuan/miniconda3/envs/udl/lib/python3.13/site-packages (from matplotlib->mnist1d==0.0.2.post16) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /Users/chengjunyuan/miniconda3/envs/udl/lib/python3.13/site-packages (from matplotlib->mnist1d==0.0.2.post16) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/chengjunyuan/miniconda3/envs/udl/lib/python3.13/site-packages (from matplotlib->mnist1d==0.0.2.post16) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/chengjunyuan/miniconda3/envs/udl/lib/python3.13/site-packages (from matplotlib->mnist1d==0.0.2.post16) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /Users/chengjunyuan/miniconda3/envs/udl/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib->mnist1d==0.0.2.post16) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/chengjunyuan/miniconda3/envs/udl/lib/python3.13/site-packages (from requests->mnist1d==0.0.2.post16) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/chengjunyuan/miniconda3/envs/udl/lib/python3.13/site-packages (from requests->mnist1d==0.0.2.post16) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/chengjunyuan/miniconda3/envs/udl/lib/python3.13/site-packages (from requests->mnist1d==0.0.2.post16) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/chengjunyuan/miniconda3/envs/udl/lib/python3.13/site-packages (from requests->mnist1d==0.0.2.post16) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "# Run this if you're in a Colab to install MNIST 1D repository\n",
        "!pip install git+https://github.com/greydanus/mnist1d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hFxuHpRqTgri"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import mnist1d\n",
        "import random\n",
        "random.seed(0)\n",
        "\n",
        "# Try attaching to GPU -- Use \"Change Runtime Type to change to GPUT\"\n",
        "DEVICE = str(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "print('Using:', DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PW2gyXL5UkLU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Did or could not load data from ./mnist1d_data.pkl. Rebuilding dataset...\n",
            "Examples in training set: 4000\n",
            "Examples in test set: 4000\n",
            "Dimensionality of each example: 40\n"
          ]
        }
      ],
      "source": [
        "args = mnist1d.data.get_dataset_args()\n",
        "args.num_samples = 8000\n",
        "args.train_split = 0.5\n",
        "args.corr_noise_scale = 0.25\n",
        "args.iid_noise_scale=2e-2\n",
        "data = mnist1d.data.get_dataset(args, path='./mnist1d_data.pkl', download=False, regenerate=True)\n",
        "\n",
        "# Add 15% noise to training labels\n",
        "for c_y in range(len(data['y'])):\n",
        "    random_number = random.random()\n",
        "    if random_number < 0.15 :\n",
        "        random_int = int(random.random() * 10)\n",
        "        data['y'][c_y] = random_int\n",
        "\n",
        "# The training and test input and outputs are in\n",
        "# data['x'], data['y'], data['x_test'], and data['y_test']\n",
        "print(\"Examples in training set: {}\".format(len(data['y'])))\n",
        "print(\"Examples in test set: {}\".format(len(data['y_test'])))\n",
        "print(\"Dimensionality of each example: {}\".format(data['x'].shape[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hAIvZOAlTnk9"
      },
      "outputs": [],
      "source": [
        "# Initialize the parameters with He initialization\n",
        "def weights_init(layer_in):\n",
        "  if isinstance(layer_in, nn.Linear):\n",
        "    nn.init.kaiming_uniform_(layer_in.weight)\n",
        "    layer_in.bias.data.fill_(0.0)\n",
        "\n",
        "# Return an initialized model with two hidden layers and n_hidden hidden units at each\n",
        "def get_model(n_hidden):\n",
        "\n",
        "  D_i = 40    # Input dimensions\n",
        "  D_k = n_hidden   # Hidden dimensions\n",
        "  D_o = 10    # Output dimensions\n",
        "\n",
        "  # Define a model with two hidden layers\n",
        "  # And ReLU activations between them\n",
        "  model = nn.Sequential(\n",
        "  nn.Linear(D_i, D_k),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(D_k, D_k),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(D_k, D_o))\n",
        "\n",
        "  # Call the function you just defined\n",
        "  model.apply(weights_init)\n",
        "\n",
        "  # Return the model\n",
        "  return model ;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AazlQhheWmHk"
      },
      "outputs": [],
      "source": [
        "def fit_model(model, data, n_epoch):\n",
        "\n",
        "  # choose cross entropy loss function (equation 5.24)\n",
        "  loss_function = torch.nn.CrossEntropyLoss()\n",
        "  # construct SGD optimizer and initialize learning rate and momentum\n",
        "  # optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "  x_train = torch.tensor(data['x'].astype('float32'))\n",
        "  y_train = torch.tensor(data['y'].transpose().astype('long'))\n",
        "  x_test= torch.tensor(data['x_test'].astype('float32'))\n",
        "  y_test = torch.tensor(data['y_test'].astype('long'))\n",
        "\n",
        "  # load the data into a class that creates the batches\n",
        "  data_loader = DataLoader(TensorDataset(x_train,y_train), batch_size=100, shuffle=True, worker_init_fn=np.random.seed(1))\n",
        "\n",
        "  for epoch in range(n_epoch):\n",
        "    # loop over batches\n",
        "    for i, batch in enumerate(data_loader):\n",
        "      # retrieve inputs and labels for this batch\n",
        "      x_batch, y_batch = batch\n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "      # forward pass -- calculate model output\n",
        "      pred = model(x_batch)\n",
        "      # compute the loss\n",
        "      loss = loss_function(pred, y_batch)\n",
        "      # backward pass\n",
        "      loss.backward()\n",
        "      # SGD update\n",
        "      optimizer.step()\n",
        "\n",
        "    # Run whole dataset to get statistics -- normally wouldn't do this\n",
        "    pred_train = model(x_train)\n",
        "    pred_test = model(x_test)\n",
        "    _, predicted_train_class = torch.max(pred_train.data, 1)\n",
        "    _, predicted_test_class = torch.max(pred_test.data, 1)\n",
        "    errors_train = 100 - 100 * (predicted_train_class == y_train).float().sum() / len(y_train)\n",
        "    errors_test= 100 - 100 * (predicted_test_class == y_test).float().sum() / len(y_test)\n",
        "    losses_train = loss_function(pred_train, y_train).item()\n",
        "    losses_test= loss_function(pred_test, y_test).item()\n",
        "    if epoch%100 ==0 :\n",
        "      print(f'Epoch {epoch:5d}, train loss {losses_train:.6f}, train error {errors_train:3.2f},  test loss {losses_test:.6f}, test error {errors_test:3.2f}')\n",
        "\n",
        "  return errors_train, errors_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AQNCmFNV6JpV"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcP4UPMudxPS"
      },
      "source": [
        "The following code produces the double descent curve by training the model with different numbers of hidden units and plotting the test error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "K4OmBZGHWXpk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model with   2 hidden variables\n",
            "Epoch     0, train loss 2.301687, train error 88.85,  test loss 2.303898, test error 89.47\n",
            "Epoch   100, train loss 1.868486, train error 69.70,  test loss 1.720074, test error 66.95\n",
            "Epoch   200, train loss 1.855949, train error 68.62,  test loss 1.702087, test error 66.55\n",
            "Epoch   300, train loss 1.854382, train error 68.85,  test loss 1.704355, test error 65.68\n",
            "Epoch   400, train loss 1.856044, train error 69.32,  test loss 1.707748, test error 66.18\n",
            "Epoch   500, train loss 1.854237, train error 69.15,  test loss 1.702928, test error 66.68\n",
            "Epoch   600, train loss 1.857942, train error 69.50,  test loss 1.708246, test error 66.75\n",
            "Epoch   700, train loss 1.850617, train error 68.40,  test loss 1.704601, test error 66.05\n",
            "Epoch   800, train loss 1.854199, train error 69.07,  test loss 1.711624, test error 66.43\n",
            "Epoch   900, train loss 1.851897, train error 68.57,  test loss 1.705448, test error 66.62\n",
            "Training model with   4 hidden variables\n",
            "Epoch     0, train loss 2.286008, train error 86.30,  test loss 2.281356, test error 85.90\n",
            "Epoch   100, train loss 1.781889, train error 66.35,  test loss 1.625180, test error 63.15\n",
            "Epoch   200, train loss 1.763184, train error 65.75,  test loss 1.605631, test error 62.15\n",
            "Epoch   300, train loss 1.757974, train error 65.07,  test loss 1.599704, test error 62.55\n",
            "Epoch   400, train loss 1.758034, train error 65.40,  test loss 1.604182, test error 62.22\n",
            "Epoch   500, train loss 1.759881, train error 65.60,  test loss 1.603632, test error 61.70\n",
            "Epoch   600, train loss 1.757511, train error 65.10,  test loss 1.610197, test error 62.62\n",
            "Epoch   700, train loss 1.758319, train error 66.07,  test loss 1.605995, test error 62.55\n",
            "Epoch   800, train loss 1.757188, train error 65.22,  test loss 1.610787, test error 63.03\n",
            "Epoch   900, train loss 1.755645, train error 65.22,  test loss 1.611215, test error 63.05\n",
            "Training model with   6 hidden variables\n",
            "Epoch     0, train loss 2.255089, train error 86.18,  test loss 2.246883, test error 85.07\n",
            "Epoch   100, train loss 1.686612, train error 61.28,  test loss 1.504167, test error 58.90\n",
            "Epoch   200, train loss 1.663223, train error 60.47,  test loss 1.489196, test error 57.80\n",
            "Epoch   300, train loss 1.642861, train error 59.05,  test loss 1.478706, test error 56.25\n",
            "Epoch   400, train loss 1.631235, train error 59.10,  test loss 1.488084, test error 56.67\n",
            "Epoch   500, train loss 1.620815, train error 58.25,  test loss 1.484119, test error 56.25\n",
            "Epoch   600, train loss 1.619311, train error 58.40,  test loss 1.489720, test error 56.75\n",
            "Epoch   700, train loss 1.612062, train error 58.58,  test loss 1.484945, test error 55.90\n",
            "Epoch   800, train loss 1.611975, train error 58.75,  test loss 1.501537, test error 56.35\n",
            "Epoch   900, train loss 1.604046, train error 58.08,  test loss 1.487668, test error 55.67\n",
            "Training model with   8 hidden variables\n",
            "Epoch     0, train loss 2.159283, train error 81.32,  test loss 2.127585, test error 81.07\n",
            "Epoch   100, train loss 1.607729, train error 57.03,  test loss 1.436674, test error 54.72\n",
            "Epoch   200, train loss 1.566337, train error 55.15,  test loss 1.398366, test error 52.92\n",
            "Epoch   300, train loss 1.552091, train error 54.20,  test loss 1.395800, test error 53.42\n",
            "Epoch   400, train loss 1.533773, train error 53.22,  test loss 1.394489, test error 53.08\n",
            "Epoch   500, train loss 1.517211, train error 53.05,  test loss 1.389930, test error 52.55\n",
            "Epoch   600, train loss 1.511485, train error 53.12,  test loss 1.393323, test error 52.85\n",
            "Epoch   700, train loss 1.503841, train error 52.65,  test loss 1.401747, test error 53.97\n",
            "Epoch   800, train loss 1.494210, train error 52.47,  test loss 1.418088, test error 53.80\n",
            "Epoch   900, train loss 1.491081, train error 52.00,  test loss 1.415990, test error 54.15\n",
            "Training model with  10 hidden variables\n",
            "Epoch     0, train loss 2.206962, train error 82.05,  test loss 2.196361, test error 81.68\n",
            "Epoch   100, train loss 1.526026, train error 53.12,  test loss 1.373670, test error 52.00\n",
            "Epoch   200, train loss 1.456917, train error 50.03,  test loss 1.323872, test error 50.12\n",
            "Epoch   300, train loss 1.399645, train error 47.72,  test loss 1.281153, test error 47.33\n",
            "Epoch   400, train loss 1.387326, train error 47.78,  test loss 1.282189, test error 47.62\n",
            "Epoch   500, train loss 1.378504, train error 47.17,  test loss 1.284357, test error 47.70\n",
            "Epoch   600, train loss 1.371837, train error 46.33,  test loss 1.279575, test error 47.65\n",
            "Epoch   700, train loss 1.360516, train error 46.28,  test loss 1.278962, test error 47.38\n",
            "Epoch   800, train loss 1.363002, train error 46.65,  test loss 1.283354, test error 47.28\n",
            "Epoch   900, train loss 1.365703, train error 45.88,  test loss 1.286032, test error 47.08\n",
            "Training model with  14 hidden variables\n",
            "Epoch     0, train loss 2.185772, train error 83.20,  test loss 2.152358, test error 82.60\n",
            "Epoch   100, train loss 1.428565, train error 47.88,  test loss 1.325079, test error 49.08\n",
            "Epoch   200, train loss 1.348461, train error 44.83,  test loss 1.306096, test error 48.62\n",
            "Epoch   300, train loss 1.318929, train error 44.72,  test loss 1.316831, test error 48.58\n",
            "Epoch   400, train loss 1.299664, train error 43.88,  test loss 1.330618, test error 48.95\n",
            "Epoch   500, train loss 1.291352, train error 43.15,  test loss 1.353154, test error 49.30\n",
            "Epoch   600, train loss 1.275364, train error 43.47,  test loss 1.356257, test error 48.78\n",
            "Epoch   700, train loss 1.271216, train error 43.20,  test loss 1.370470, test error 49.20\n",
            "Epoch   800, train loss 1.267597, train error 42.65,  test loss 1.382547, test error 49.38\n",
            "Epoch   900, train loss 1.252323, train error 42.03,  test loss 1.396213, test error 50.22\n",
            "Training model with  18 hidden variables\n",
            "Epoch     0, train loss 2.236383, train error 83.07,  test loss 2.245158, test error 84.15\n",
            "Epoch   100, train loss 1.279974, train error 43.03,  test loss 1.267414, test error 48.35\n",
            "Epoch   200, train loss 1.196328, train error 40.35,  test loss 1.316895, test error 47.97\n",
            "Epoch   300, train loss 1.163870, train error 39.58,  test loss 1.347595, test error 48.62\n",
            "Epoch   400, train loss 1.152001, train error 39.60,  test loss 1.373691, test error 48.75\n",
            "Epoch   500, train loss 1.134683, train error 38.67,  test loss 1.400140, test error 49.55\n",
            "Epoch   600, train loss 1.121301, train error 38.15,  test loss 1.431774, test error 50.40\n",
            "Epoch   700, train loss 1.135894, train error 39.10,  test loss 1.451090, test error 50.10\n",
            "Epoch   800, train loss 1.117357, train error 38.17,  test loss 1.458506, test error 49.53\n",
            "Epoch   900, train loss 1.111418, train error 38.53,  test loss 1.475757, test error 49.78\n",
            "Training model with  22 hidden variables\n",
            "Epoch     0, train loss 2.189616, train error 82.07,  test loss 2.158711, test error 82.20\n",
            "Epoch   100, train loss 1.291432, train error 43.47,  test loss 1.360756, test error 51.00\n",
            "Epoch   200, train loss 1.162480, train error 39.50,  test loss 1.415977, test error 51.30\n",
            "Epoch   300, train loss 1.107484, train error 37.58,  test loss 1.501586, test error 51.97\n",
            "Epoch   400, train loss 1.067692, train error 35.85,  test loss 1.545874, test error 51.95\n",
            "Epoch   500, train loss 1.041265, train error 35.50,  test loss 1.628796, test error 52.45\n",
            "Epoch   600, train loss 1.015944, train error 34.35,  test loss 1.690555, test error 53.12\n",
            "Epoch   700, train loss 0.996653, train error 33.72,  test loss 1.736663, test error 53.50\n",
            "Epoch   800, train loss 0.991935, train error 33.75,  test loss 1.805596, test error 53.88\n",
            "Epoch   900, train loss 0.989945, train error 33.25,  test loss 1.856581, test error 54.40\n",
            "Training model with  26 hidden variables\n",
            "Epoch     0, train loss 2.117199, train error 79.28,  test loss 2.075975, test error 78.80\n",
            "Epoch   100, train loss 1.188078, train error 39.85,  test loss 1.328484, test error 49.83\n",
            "Epoch   200, train loss 1.045035, train error 35.35,  test loss 1.396744, test error 49.85\n",
            "Epoch   300, train loss 0.982275, train error 34.20,  test loss 1.512672, test error 50.92\n",
            "Epoch   400, train loss 0.930443, train error 31.57,  test loss 1.619179, test error 51.92\n",
            "Epoch   500, train loss 0.908505, train error 31.20,  test loss 1.722509, test error 52.72\n",
            "Epoch   600, train loss 0.879545, train error 30.05,  test loss 1.783831, test error 53.08\n",
            "Epoch   700, train loss 0.863535, train error 29.68,  test loss 1.878352, test error 53.55\n",
            "Epoch   800, train loss 0.844153, train error 28.85,  test loss 1.928679, test error 53.58\n",
            "Epoch   900, train loss 0.823583, train error 28.20,  test loss 1.983201, test error 54.33\n",
            "Training model with  30 hidden variables\n",
            "Epoch     0, train loss 2.232642, train error 82.32,  test loss 2.226342, test error 82.62\n",
            "Epoch   100, train loss 1.131883, train error 35.38,  test loss 1.317904, test error 47.92\n",
            "Epoch   200, train loss 0.976024, train error 31.65,  test loss 1.481110, test error 50.88\n",
            "Epoch   300, train loss 0.887471, train error 28.62,  test loss 1.674201, test error 53.15\n",
            "Epoch   400, train loss 0.833455, train error 26.97,  test loss 1.838041, test error 53.50\n",
            "Epoch   500, train loss 0.781917, train error 24.90,  test loss 1.997622, test error 53.92\n",
            "Epoch   600, train loss 0.761767, train error 26.35,  test loss 2.158067, test error 54.72\n",
            "Epoch   700, train loss 0.739135, train error 24.40,  test loss 2.269827, test error 54.85\n",
            "Epoch   800, train loss 0.713753, train error 23.90,  test loss 2.408698, test error 55.17\n",
            "Epoch   900, train loss 0.704956, train error 23.32,  test loss 2.558171, test error 55.70\n",
            "Training model with  35 hidden variables\n",
            "Epoch     0, train loss 2.135546, train error 80.38,  test loss 2.114666, test error 79.80\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m total_weights_all[c_hidden] = count_parameters(model)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m errors_train, errors_test = \u001b[43mfit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Store the results\u001b[39;00m\n\u001b[32m     22\u001b[39m errors_train_all[c_hidden] = errors_train\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mfit_model\u001b[39m\u001b[34m(model, data, n_epoch)\u001b[39m\n\u001b[32m     24\u001b[39m optimizer.zero_grad()\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# forward pass -- calculate model output\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# compute the loss\u001b[39;00m\n\u001b[32m     28\u001b[39m loss = loss_function(pred, y_batch)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/udl/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/udl/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/udl/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/udl/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/udl/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/udl/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# This code will take a while (~30 mins on GPU) to run!  Go and make a cup of coffee!\n",
        "\n",
        "hidden_variables = np.array([2,4,6,8,10,14,18,22,26,30,35,40,45,50,55,60,70,80,90,100,120,140,160,180,200,250,300,400]) ;\n",
        "\n",
        "errors_train_all = np.zeros_like(hidden_variables)\n",
        "errors_test_all = np.zeros_like(hidden_variables)\n",
        "total_weights_all = np.zeros_like(hidden_variables)\n",
        "\n",
        "# loop over the dataset n_epoch times\n",
        "n_epoch = 1000\n",
        "\n",
        "# For each hidden variable size\n",
        "for c_hidden in range(len(hidden_variables)):\n",
        "    print(f'Training model with {hidden_variables[c_hidden]:3d} hidden variables')\n",
        "    # Get a model\n",
        "    model = get_model(hidden_variables[c_hidden]) ;\n",
        "    # Count and store number of weights\n",
        "    total_weights_all[c_hidden] = count_parameters(model)\n",
        "    # Train the model\n",
        "    errors_train, errors_test = fit_model(model, data, n_epoch)\n",
        "    # Store the results\n",
        "    errors_train_all[c_hidden] = errors_train\n",
        "    errors_test_all[c_hidden]= errors_test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Rw-iRboTXbck"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming data['y'] is available and contains the training examples\n",
        "num_training_examples = len(data['y'])\n",
        "\n",
        "# Find the index where total_weights_all is closest to num_training_examples\n",
        "closest_index = np.argmin(np.abs(np.array(total_weights_all) - num_training_examples))\n",
        "\n",
        "# Get the corresponding value of hidden variables\n",
        "hidden_variable_at_num_training_examples = hidden_variables[closest_index]\n",
        "\n",
        "# Plot the results\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(hidden_variables, errors_train_all, 'r-', label='train')\n",
        "ax.plot(hidden_variables, errors_test_all, 'b-', label='test')\n",
        "\n",
        "# Add a vertical line at the point where total weights equal the number of training examples\n",
        "ax.axvline(x=hidden_variable_at_num_training_examples, color='g', linestyle='--', label='N(weights) = N(train)')\n",
        "\n",
        "ax.set_ylim(0, 100)\n",
        "ax.set_xlabel('No. hidden variables')\n",
        "ax.set_ylabel('Error')\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "udl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
